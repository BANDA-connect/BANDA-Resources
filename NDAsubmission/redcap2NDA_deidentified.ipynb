{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e80530-671b-44fa-8890-193113458e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as  pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import dateutil.parser\n",
    "import collections\n",
    "import re\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dba8bc-fad9-4b47-8b63-bc05ed3da67c",
   "metadata": {},
   "source": [
    "#### Read Redcap data into Pandas dataframe\n",
    "\n",
    "The Redcap data were stored in different projects, and exported in different formats. \n",
    "This section reads all the RedCap data into a single Pandas dataframe that forms the \n",
    "basis of all further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1c3055-4a00-42b1-a011-35a21270d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "redcap = Path(\"../../../../../BANDA215/Redcap2022\")\n",
    "df_male = pd.read_table(redcap / \"BANDAAdolescentMale_DATA_2022-01-26_1933.csv\")\n",
    "df_female = pd.read_csv(redcap / \"BANDAAdolescentFemal_DATA_2022-01-26_1932.csv\")\n",
    "df_parents = pd.read_table(redcap / \"BANDAParent_DATA_2022-01-26_1933.csv\")\n",
    "df_ids = pd.read_table(redcap / \"BANDAData_DATA_2022-01-26_1936.csv\")\n",
    "df_clinics = pd.read_table(redcap / \"BANDAClinicalIntervi_DATA_2022-01-26_1934.csv\",\n",
    "                          na_values=[\"n/a\"])\n",
    "\n",
    "df_male.columns = [var if \"subject\" not in var else \"subject_id\" for var in df_male.columns]\n",
    "df_parents.columns = [var if \"subject\" not in var else \"subject_id\" for var in df_parents.columns]\n",
    "df_parents.drop(df_parents[df_parents.parent_initial_timestamp == '[not completed]'].index, inplace=True)\n",
    "\n",
    "df_all = pd.concat([df_male, df_female], axis=0, ignore_index=True)\n",
    "\n",
    "subjmap = {}\n",
    "df_subjmap = pd.read_csv(\"nda-subject-map.csv\")\n",
    "for val in df_subjmap.iterrows():\n",
    "    subjmap[val[1].src_subject_id] = val[1].subjectkey\n",
    "subjmap_rev = {v:k for k,v in subjmap.items()}\n",
    "\n",
    "# Map NDA Pseudo GUIDS to local subject identifiers\n",
    "old_subj_info = pd.read_csv(\"../../../../C3037 - Mappings/c3037_ndar_subject_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7f222b-2da4-4a18-832c-b34ebd9f8f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redcap stores different timepoints of a longitudinal survey in variables with appended suffixes\n",
    "suffixmap = {\"_i\": \"T1\", \"_1\": \"T2\", \"_2\": \"T3\", \"_3\": \"T4\"}\n",
    "# map visits to keys\n",
    "timemap = {\"_initial_\": \"T1\", \"_6_\": \"T2\", \"_12_\": \"T3\", \"_18_\": \"T4\"}\n",
    "nda_vars = ['src_subject_id', 'subject_id', 'visit', 'interview_date']\n",
    "\n",
    "def get_timemap(df, regex):\n",
    "    \"\"\"Given a regex this function uses relevant timestamp columns to map to visits\"\"\"\n",
    "    timestamps = [var for var in df.columns if re.match(f\"{regex}timestamp$\", var)]\n",
    "    #print(timestamps)\n",
    "    ts2period = {}\n",
    "    for loc in timestamps:\n",
    "        for point in timemap:\n",
    "            if point in loc:\n",
    "                ts2period[loc] = timemap[point]\n",
    "                continue\n",
    "    ts2period_rev = {v:k for k,v in ts2period.items()}\n",
    "    suffixmap_ts = {k:ts2period_rev[v] for k, v in suffixmap.items() if v in ts2period_rev}\n",
    "    suffixmap_filt = {k:v for k, v in suffixmap.items() if v in ts2period_rev}\n",
    "    return ts2period, ts2period_rev, suffixmap_filt, suffixmap_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f6780f-694b-4bd9-aa9a-89638c4f830a",
   "metadata": {},
   "source": [
    "#### Create dictionary of mappings from Redcap variables to NDA data elements\n",
    "\n",
    "The mappings file also encodes transformations necessary to convert BANDA Redcap\n",
    "variables to the corresponding NDA data elements. These mappings are provided\n",
    "in the crosswalk file from the NDA study downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf2b458-240c-4d0f-9682-225d3a26dca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_maps = []\n",
    "var_map = {}\n",
    "assess_map = {}\n",
    "for fname in glob(\"../../../../C3037 - Mappings/*mappings.csv\"):\n",
    "    df_map = pd.read_csv(fname)\n",
    "    print(Path(fname).name, df_map[[\"assessment\", \"ndar_structure\"]].drop_duplicates().values.tolist())\n",
    "    df_maps.append(df_map)\n",
    "    for var in df_map.banda_var.unique():\n",
    "        var_map[var] = len(df_maps) - 1\n",
    "    for assess in df_map.ndar_structure.dropna().unique():\n",
    "        assess_map[assess] = len(df_maps) - 1\n",
    "    if 'na_val' in df_map.columns:\n",
    "        print(\"Missing values: \", df_map.na_val.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807e2a33-ba00-47d8-bdd6-b8a57e0bb286",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(df_maps, axis=0).to_csv(\"Crosswalk.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ef7b1-522f-4b64-9d95-f7d5b8f03dcf",
   "metadata": {},
   "source": [
    "#### Transformations to apply to the data\n",
    "\n",
    "The original intent was to be able to reverse every transformation. Given the \n",
    "complications of transforming data from Redcap to NDA, only the forward \n",
    "transformations are relevant. The function names correspond to the transform\n",
    "column in the mapping file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8903a2d1-be42-4933-abee-cc45e1484fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisbas_rev(x, invert=False): \n",
    "    if x not in range(1, 5):\n",
    "        raise ValueError(f\"{x} not in [1, 4]\")\n",
    "    recode = {v: 5-v for v in range(1, 5)}\n",
    "    return recode[x]\n",
    "\n",
    "def handed(x, invert=False):\n",
    "    if not invert:\n",
    "        if x == 1: return 0\n",
    "        if x == 3: return 1\n",
    "    else:\n",
    "        if x == 1: return 3\n",
    "        if x == 0: return 1\n",
    "    return x\n",
    "\n",
    "def plus1(x, invert=False):\n",
    "    return x - 1 if invert else x + 1\n",
    "\n",
    "def plus1_rev(x, invert=False):\n",
    "    if not invert:\n",
    "        x = x + 1\n",
    "    recode = {v: 5-v for v in range(1, 5)}\n",
    "    x = recode[x]\n",
    "    if invert:\n",
    "        x = x - 1\n",
    "    return x\n",
    "\n",
    "def minus1(x, invert=False):\n",
    "    return x + 1 if invert else x - 1\n",
    "\n",
    "def marital(x, invert=False):\n",
    "    remap = {0:8, 1:6, 2:5, 3:4, 4:7}\n",
    "    if invert:\n",
    "        remap_rev = {v:k for k,v in remap.items()}\n",
    "        return remap_rev[x] if x in remap_rev else x\n",
    "    else:\n",
    "        return remap[x] if x in remap else x\n",
    "\n",
    "def TOSTRING(x, invert=False):\n",
    "    if invert:\n",
    "        raise Exception(\"Cannot invert\")\n",
    "    masqmap = {1: \"not at all\",\n",
    "             2: \"a little bit\",\n",
    "             3: \"moderately\",\n",
    "             4: \"quite a bit\",\n",
    "             5: \"extremely\"}\n",
    "    return masqmap[x]\n",
    "\n",
    "def REVERSE(x, invert=False):\n",
    "    if x not in range(1, 6):\n",
    "        raise ValueError(f\"{x} not in [1, 6]\")\n",
    "    recode = {x: 6-x for x in range(1, 6)}\n",
    "    return recode[x]\n",
    "\n",
    "def rmbi_rev(x, invert=False):\n",
    "    if x not in range(4):\n",
    "        ValueError(f\"{x} not in [0, 3]\")\n",
    "    recode = {x: 3-x for x in range(4)}\n",
    "    return recode[x]\n",
    "\n",
    "def cssrs(x, invert=False):\n",
    "    if invert:\n",
    "        if x == 2: return 0\n",
    "    else:\n",
    "        if x == 0: return 2\n",
    "    return x\n",
    "\n",
    "def age(x, invert=False):\n",
    "    if invert:\n",
    "        return x/12\n",
    "    return x*12\n",
    "\n",
    "def fhs(x, invert=False):\n",
    "    if invert:\n",
    "        raise Exception(\"Cannot invert\")\n",
    "    if x == 9:\n",
    "        return np.nan\n",
    "    return x\n",
    "    \n",
    "def ksads(x, invert=False):\n",
    "    if x == 3: return 4\n",
    "    if x == 4: return 3\n",
    "    return x\n",
    "\n",
    "def map_race(x, invert=False):\n",
    "    racemap = {0: \"American Indian/Alaska Native\",\n",
    "               1: \"Asian\",\n",
    "               2: \"Hawaiian or Pacific Islander\",\n",
    "               3: \"Black or African American\", \n",
    "               4: \"White\", \n",
    "               5: \"More than one race\", \n",
    "               6: \"Unknown or not reported\"}\n",
    "    racemap_rev = {v:k for k,v in racemap.items()}\n",
    "    if invert:\n",
    "        raise Exception(\"Cannot invert\")\n",
    "    else:\n",
    "        races = []\n",
    "        for val in np.where(x)[0].tolist():\n",
    "            races.append(racemap[val])\n",
    "        if len(races) == 0:\n",
    "            return \"Unknown or not reported\"\n",
    "        if len(races) > 1:\n",
    "            return \"More than one race\"\n",
    "        return races.pop()\n",
    "\n",
    "def map_sex(x, invert=False):\n",
    "    sexmap = {0: \"Male\",\n",
    "              1: \"Female\"}\n",
    "    sexmap_rev = {v:k for k,v in sexmap.items()}\n",
    "    if invert:\n",
    "        return sexmap_rev[x]\n",
    "    else:\n",
    "        return sexmap[x]\n",
    "\n",
    "def map_ethnicity(x, invert=False):\n",
    "    ethnicmap = {0: \"Not Hispanic or Latino\",\n",
    "                 1: \"Hispanic or Latino\"}\n",
    "    ethnicmap_rev = {v:k for k,v in ethnicmap.items()}\n",
    "    if invert:\n",
    "        return ethnicmap_rev[x]\n",
    "    else:\n",
    "        return ethnicmap[x]\n",
    "\n",
    "def map_education(x, invert=False):\n",
    "    edumap = {0: \"8th Grade or Less\",\n",
    "              1: \"Some High School\",\n",
    "              2: \"Finished High School\",\n",
    "              3: \"Completed GED\",\n",
    "              4: \"Vocation/Trade/Business School\",\n",
    "              5: \"Some College or 2 year degree\",\n",
    "              6: \"Finished 4 year degree\",\n",
    "              7: \"Master's degree or equivalent\",\n",
    "              8: \"Other Advanced degree\",\n",
    "              9: \"Unknown\"}\n",
    "    edumap_rev = {v:k for k,v in edumap.items()}\n",
    "    if invert:\n",
    "        return edumap_rev[x]\n",
    "    else:\n",
    "        return edumap[x]\n",
    "\n",
    "def map_income(x, invert=False):\n",
    "    incmap = {0: 3, #\"Gross earnings (self)\"\n",
    "              1: 8, #\"Other income\"\n",
    "              2: 8, #\"Other income\"\n",
    "              3: 2, #\"Disability income\"\n",
    "              4: 8, #\"Other income\"\n",
    "             }\n",
    "    if invert: \n",
    "        raise Exception(\"Cannot invert\")\n",
    "    incomes = []\n",
    "    for val in np.where(x)[0].tolist():\n",
    "        incomes.append(incmap[val])\n",
    "    if len(incomes) == 0:\n",
    "        return np.nan\n",
    "    if len(incomes) == 1:\n",
    "        return incomes[0]\n",
    "    return 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b41b77-a308-42f5-8195-fcca8dbd94df",
   "metadata": {},
   "source": [
    "#### Find BANDA subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a76a6-4134-4db9-a892-21737f971cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = df_all.subject_id.dropna().apply(lambda x: np.nan if \"ineligible\" in x \n",
    "                                 or'-' in x \n",
    "                                 or int(x) > 900 \n",
    "                                 or int(x) < 100\n",
    "                                 else int(x)).astype(pd.Int64Dtype()).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d9c732-6978-499e-b6dd-e9b0aa9a3dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "banda_subs = df_ids[df_ids.subject_id.isin(subs)].banda_id.dropna().str.contains('BANDA')\n",
    "banda_subs_idx = banda_subs[banda_subs].index\n",
    "\n",
    "banda_df_ids = df_ids.loc[banda_subs_idx]\n",
    "sub_ids = banda_df_ids.subject_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97857e17-86b5-408e-a78f-c4d23a3475b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subs = subs.isin(banda_df_ids.subject_id)\n",
    "df_subs_idx = df_subs[df_subs].index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7759bc77-acb2-4730-8c53-bb935358cd70",
   "metadata": {},
   "source": [
    "#### Create a BANDA specific dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aea25f-ed18-4003-8930-12d07b52f367",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_banda = df_all.loc[df_subs_idx]\n",
    "df_banda[\"src_subject_id\"] = df_banda.subject_id.dropna().apply(lambda x: banda_df_ids[banda_df_ids.subject_id == int(x)].banda_id.values[0])\n",
    "\n",
    "# combine tanner girls and boys for processing\n",
    "tanner_cols = [var.replace(\"_boys\", \"\").replace(\"_girls\", \"\") for var in df_banda.columns if re.match(\"tanner\\d+_(boys|girls)\", var)]\n",
    "for val in list(sorted(set(tanner_cols))):\n",
    "    df_banda[val] = df_banda[val.replace(\"_\", \"_boys_\")].add(df_banda[val.replace(\"_\", \"_girls_\")], fill_value=0)\n",
    "df_banda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0809e98-523c-45d2-936e-5e9522cad2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_timestamp(x):\n",
    "    \"\"\"Convert encoded timestamps to a standard form\"\"\"\n",
    "    if '[' in x:\n",
    "        return np.nan\n",
    "    #if \":\" in x or '-' in x:\n",
    "    return dateutil.parser.parse(x)\n",
    "    #return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d3743-7c9f-4ad3-84fd-74fd969dc6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess2df(df, assess, ts_regex, recode=True, skip_suffix=False):\n",
    "    \"\"\"Convert each assessment to a dataframe\n",
    "    \n",
    "    This function also recodes values by applying the relevant transforms.\n",
    "   \n",
    "    \"\"\"\n",
    "    _, _, suffixmap, suffixmap_ts = get_timemap(df, ts_regex)\n",
    "    df_assess = []\n",
    "    for tp, tcol in suffixmap_ts.items():\n",
    "        tpext = tp\n",
    "        if skip_suffix:\n",
    "            tpext = \"\"\n",
    "        if isinstance(assess, str):\n",
    "            df_assess.append(pd.concat([df[[\"src_subject_id\", \"subject_id\", tcol]], \n",
    "                                        df.filter(regex=f\"{assess}{tpext}$\")], axis=1))\n",
    "        elif isinstance(assess, list):\n",
    "            df_assess.append(pd.concat([df[[\"src_subject_id\", \"subject_id\", tcol]], \n",
    "                                        df.filter(items=[f\"{var}{tpext}\" for var in assess])], axis=1))\n",
    "        else:\n",
    "            raise ValueError(\"assess parameter must be a regex or a list\")\n",
    "        df_assess[-1][\"visit\"] = suffixmap[tp]\n",
    "        df_assess[-1][\"interview_date\"] = df_assess[-1][tcol].dropna().apply(lambda x: to_timestamp(x))\n",
    "        df_assess[-1] = df_assess[-1].drop(tcol, axis=1)\n",
    "        if not skip_suffix:\n",
    "            columns = [x.removesuffix(tp) for x in df_assess[-1].columns]\n",
    "            df_assess[-1].columns = columns\n",
    "        df_assess[-1].sort_values(\"src_subject_id\", inplace=True)\n",
    "        df_assess[-1] = df_assess[-1].reset_index(drop=True)\n",
    "    if len(df_assess) > 1:\n",
    "        df_all = pd.concat(df_assess, axis=0, ignore_index=True)\n",
    "    else:\n",
    "        df_all = df_assess[0]\n",
    "    if recode:\n",
    "        for var in df_all:\n",
    "            if var in var_map:\n",
    "                varinfo = df_maps[var_map[var]][df_maps[var_map[var]].banda_var == var]\n",
    "                if len(varinfo.recode.dropna()):\n",
    "                    recode_func = globals()[varinfo.recode.values[0]]\n",
    "                    df_all[var] = df_all[var].dropna().apply(lambda x: recode_func(x))\n",
    "                if len(varinfo.na_val.dropna()):\n",
    "                    if varinfo.na_val.values[0] in df_all[var].tolist():\n",
    "                        df_all[var].loc[df_all[var] == varinfo.na_val.values[0], var] = np.nan\n",
    "                if varinfo.ndar_var.values[0] != var:\n",
    "                    df_all = df_all.rename(columns={var: varinfo.ndar_var.values[0]})\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8ee828-7450-4757-b4e8-a3d09476e067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_allnans(df, cols):\n",
    "    \"\"\"Drop data connecting NaNs\"\"\"\n",
    "    allnans = df[cols].isnull().all(axis=1)\n",
    "    somedata = allnans[allnans == False]\n",
    "    return df.loc[somedata.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec0d31d-e921-4d22-8ce8-654e21977b28",
   "metadata": {},
   "source": [
    "#### Process the adolescent assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe42c41d-d021-4a04-885d-e8ff565722e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_regex = \"adolescent.*\"\n",
    "print(get_timemap(df_banda, ts_regex))\n",
    "dfs_assess = {}\n",
    "for assess in [\"bisbas\",\n",
    "               \"handed\",\n",
    "               \"mfq\", \n",
    "               \"neo\", \n",
    "               \"rbqa\", \n",
    "               \"rcads\",\n",
    "               \"shaps\", \n",
    "               \"stai_(state|trait)\", \n",
    "               \"tanner\"]:\n",
    "    assess_strip = assess.split(\"\\\\\")[0].split(\".\")[0].split(\"_\")[0]\n",
    "    dfs_assess[assess_strip] = assess2df(df_banda, f\"{assess}\\d+\", ts_regex)\n",
    "    cols = np.setdiff1d(dfs_assess[assess_strip].columns, nda_vars)\n",
    "    dfs_assess[assess_strip] = drop_allnans(dfs_assess[assess_strip], cols)\n",
    "    dfs_assess[assess_strip][\"respondent\"] = \"Child\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d59731e-1e1f-4461-a6a5-59e74ddb95c4",
   "metadata": {},
   "source": [
    "#### Process the parent assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56379558-895c-44b9-9f3d-6c1092408ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_subs = df_parents.subject_id.dropna().apply(lambda x: np.nan if \"ineligible\" in x \n",
    "                                                   or '-' in x \n",
    "                                                   or 'HP' in x\n",
    "                                                   or int(x) > 900 \n",
    "                                                   or int(x) < 100\n",
    "                                                   else int(x)).astype(pd.Int64Dtype()).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204084b8-765d-4c9e-9e87-063629875949",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parent_subs = parent_subs.isin(sub_ids)\n",
    "df_parent_subs_idx = df_parent_subs[df_parent_subs].index\n",
    "df_parent = df_parents.loc[df_parent_subs_idx]\n",
    "df_parent.subject_id = df_parent.subject_id.apply(lambda x: int(x))\n",
    "df_parent[\"src_subject_id\"] = df_parent.subject_id.dropna().apply(lambda x: banda_df_ids[banda_df_ids.subject_id == int(x)].banda_id.values[0])\n",
    "\n",
    "# collapse categorical variables\n",
    "catvars = np.unique([var.split(\"__\")[0] for var in df_parent.columns if \"__\" in var]).tolist()\n",
    "for catvar in catvars:\n",
    "    varcols = sorted([var for var in df_parent.columns if var.startswith(catvar + \"__\")])\n",
    "    df_parent[catvar] = df_parent[varcols].values.tolist() #.dot([2** var for var in range(len(varcols))])\n",
    "\n",
    "df_parent.drop(columns=['demo_child_gender_i', 'demo_other_source_i', 'demo_income_i', 'demo_child_i'],\n",
    "               inplace=True)\n",
    "df_parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb25ecc-bbdf-4699-bf26-8886aa6f1630",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_regex_par = \"parent.*\"\n",
    "print(get_timemap(df_parent, ts_regex_par))\n",
    "\n",
    "dfs_assess_par = {}\n",
    "for assess in [\"cbcl.*pr\\d+([a-z]*)\", #child\n",
    "               \"demo.*(?<!__\\d)$\", \n",
    "               \"masq\\d+\", #self\n",
    "               \"mfq.*pr\\d+\", #child\n",
    "               \"rmbi.*pr\\d+\", #child\n",
    "               \"stai.*pr\\d+\"]: #self\n",
    "    assess_strip = assess.split(\"\\\\\")[0].split(\".\")[0]\n",
    "    if assess_strip == \"demo\":\n",
    "        dfs_assess_par[assess_strip] = assess2df(df_parent, assess, ts_regex_par, skip_suffix=True)\n",
    "    else:\n",
    "        dfs_assess_par[assess_strip] = assess2df(df_parent, assess, ts_regex_par)\n",
    "    cols = np.setdiff1d(dfs_assess_par[assess_strip].columns, nda_vars)\n",
    "    dfs_assess_par[assess_strip] = drop_allnans(dfs_assess_par[assess_strip], cols)\n",
    "    dfs_assess_par[assess_strip][\"respondent\"] = \"Parent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a93127-87c2-403a-9729-b341f89b8b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_assess_par[\"demo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eddacb4-e9a1-4cac-96d7-0a858687d544",
   "metadata": {},
   "source": [
    "#### Process all the clinical assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1bfdc8-3814-47c8-b0b9-c282ddf3fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinic_subs = df_clinics.subject_id.dropna().apply(lambda x: np.nan if \"ineligible\" in x.lower() \n",
    "                                                   or '-' in x \n",
    "                                                   or 'HP' in x\n",
    "                                                   or 'illegible' in x\n",
    "                                                   or int(x) > 900 \n",
    "                                                   or int(x) < 100\n",
    "                                                   else int(x)).astype(pd.Int64Dtype()).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24761d44-9372-4f25-b749-613afe84b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clinic_subs = clinic_subs.isin(sub_ids)\n",
    "df_clinic_subs_idx = df_clinic_subs[df_clinic_subs].index\n",
    "df_clinic = df_clinics.loc[df_clinic_subs_idx]\n",
    "df_clinic.subject_id = df_clinic.subject_id.apply(lambda x: int(x))\n",
    "df_clinic[\"src_subject_id\"] = df_clinic.subject_id.dropna().apply(lambda x: banda_df_ids[banda_df_ids.subject_id == int(x)].banda_id.values[0])\n",
    "df_clinic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c25d5-e17f-4b53-9aa3-d455cb768a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ksads1_var = df_maps[-1][(df_maps[-1].assessment == 'KSADS') & (df_maps[-1].ndar_structure == 'ksads_diagnoses01')].banda_var.dropna().unique().tolist()\n",
    "ksads2_var = df_maps[-1][(df_maps[-1].assessment == 'KSADS') & (df_maps[-1].ndar_structure == 'ksads_diagnosesp201')].banda_var.dropna().unique().tolist()\n",
    "\n",
    "df_other = {}\n",
    "df_ksads1 = assess2df(df_clinic, ksads1_var, \"ksads.*\")\n",
    "cols = np.setdiff1d(df_ksads1.columns, nda_vars)\n",
    "df_ksads1 = drop_allnans(df_ksads1, cols)\n",
    "dsm_drop_vars = [var for var in df_ksads1.columns if (var + \"_dsm5\") in df_ksads1.columns] + \\\n",
    "        [\"bipolarunspecpast_dsm5\", \"briefreactivepsychosiscurrent\", \"depressunspecpast_dsm5\", \n",
    "         \"schziophreniacurrent\", \"schziophreniformcurrent\"]\n",
    "df_ksads1 = df_ksads1.drop(dsm_drop_vars, axis=1)\n",
    "df_other[\"ksads_diagnoses01\"] = df_ksads1\n",
    "\n",
    "df_ksads2 = assess2df(df_clinic, ksads2_var, \"ksads.*\")\n",
    "cols = np.setdiff1d(df_ksads2.columns, nda_vars)\n",
    "df_ksads2 = drop_allnans(df_ksads2, cols)\n",
    "df_other[\"ksads_diagnosesp201\"] = df_ksads2\n",
    "    \n",
    "df_fhs = assess2df(df_clinic, \"fhs\\d+.*\", \"fhs.*\")\n",
    "cols = np.setdiff1d(df_fhs.columns, nda_vars)\n",
    "df_fhs = drop_allnans(df_fhs, cols)\n",
    "df_other[\"fhs\"] = df_fhs\n",
    "\n",
    "df_cssrs = assess2df(df_clinic, \"cssrs_.*\", \"cssrs.*\")\n",
    "describe_vars = [var for var in df_cssrs.columns if \"describe\" in var]\n",
    "df_cssrs = df_cssrs.drop(describe_vars, axis=1)\n",
    "cols = np.setdiff1d(df_cssrs.columns, nda_vars)\n",
    "df_cssrs = drop_allnans(df_cssrs, cols)\n",
    "unk_cols = [var for var in df_cssrs.columns if \"_sincelastvisit\" in var] + \\\n",
    "        [\"cssrs_ideation_severe_intensity_recent\", \"cssrs_ideation_severe_desc_recent\",\n",
    "         \"cssrs_ideation_duration_recent\", \"cssrs_ideation_control_recent\", \n",
    "         \"cssrs_ideation_deter_recent\", \"cssrs_ideation_reason_recent\",\n",
    "         \"cssrs_actual_leth_lethal_date_recent\", \"cssrs_actual_leth_lethal_code_recent\",\n",
    "         \"cssrs_potential_leth_lethal_code_recent\", \"cssrs_case_note\"]\n",
    "df_cssrs = df_cssrs.drop(unk_cols, axis=1)\n",
    "df_other[\"cssrs\"] = df_cssrs\n",
    "\n",
    "wasi_var = df_maps[-1][(df_maps[-1].assessment == 'WASI') & (df_maps[-1].ndar_structure == 'wasi201')].banda_var.dropna().unique().tolist()\n",
    "wasi_c_var = [var for var in wasi_var if not var.endswith(\"_p\")]\n",
    "wasi_p_var = [var for var in wasi_var if var.endswith(\"_p\")]\n",
    "wasi_c_var, wasi_p_var\n",
    "\n",
    "df_wasi_c = assess2df(df_clinic, wasi_c_var, \"wasi_init.*\", skip_suffix=True)\n",
    "cols = np.setdiff1d(df_wasi_c.columns, nda_vars)\n",
    "df_wasi_c = drop_allnans(df_wasi_c, cols)\n",
    "df_other[\"wasi_c\"] = df_wasi_c\n",
    "\n",
    "df_wasi_p = assess2df(df_clinic, wasi_p_var, \"wasi_parent.*\", skip_suffix=True)\n",
    "cols = np.setdiff1d(df_wasi_p.columns, nda_vars)\n",
    "df_wasi_p = drop_allnans(df_wasi_p, cols)\n",
    "df_other[\"wasi_p\"] = df_wasi_p\n",
    "\n",
    "for key in df_other:\n",
    "    df_other[key][\"respondent\"] = \"Child\" if \"_p\" not in key else \"Parent\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c1251a-e1f1-424d-b8d0-ba45a1faabfe",
   "metadata": {},
   "source": [
    "#### Create a visit structure for all timepoints and subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5121e5b5-42f4-4457-9720-cbb40a247082",
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_dates = []\n",
    "for val in dfs_assess.values():\n",
    "    visitinfo = val[[\"src_subject_id\", \"visit\", \"interview_date\"]].drop_duplicates()\n",
    "    visitinfo = visitinfo.set_index(keys=[\"src_subject_id\", \"visit\"])\n",
    "    visit_dates.append(visitinfo)\n",
    "df_visits = pd.concat(visit_dates, axis=1)\n",
    "\n",
    "visit_dates_par = []\n",
    "for val in dfs_assess_par.values():\n",
    "    visitinfo = val[[\"src_subject_id\", \"visit\", \"interview_date\"]].drop_duplicates()\n",
    "    visitinfo = visitinfo.set_index(keys=[\"src_subject_id\", \"visit\"])\n",
    "    visit_dates_par.append(visitinfo)\n",
    "df_visits_par = pd.concat(visit_dates_par, axis=1)\n",
    "\n",
    "df_visits = pd.concat(visit_dates + visit_dates_par, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd145923-54b3-4be9-93b5-3438bdde4439",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visit = df_visits.apply(lambda x: pd.to_datetime(sorted(x.dropna().unique())[0]) if len(x.dropna().unique()) >= 1 else pd.NaT, axis=1).to_frame()\n",
    "df_visit.columns = [\"interview_date\"]\n",
    "\n",
    "\n",
    "all_date_missing = df_visit[df_visit.interview_date.isnull()]\n",
    "T1_missing_loc = all_date_missing.index.get_locs([slice(None), \"T1\"])\n",
    "T1_missing_subs = all_date_missing.iloc[T1_missing_loc].index.get_level_values(\"src_subject_id\").tolist()\n",
    "T2_missing_loc = all_date_missing.index.get_locs([slice(None), \"T2\"])\n",
    "T2_missing_subs = all_date_missing.iloc[T2_missing_loc].index.get_level_values(\"src_subject_id\").tolist()\n",
    "T3_missing_loc = all_date_missing.index.get_locs([slice(None), \"T3\"])\n",
    "T3_missing_subs = all_date_missing.iloc[T3_missing_loc].index.get_level_values(\"src_subject_id\").tolist()\n",
    "\n",
    "for val in T1_missing_subs:\n",
    "    timestamp = pd.to_datetime(old_subj_info[old_subj_info.src_subject_id == val].interview_date)\n",
    "    df_visit.loc[pd.IndexSlice[(val, \"T1\")], \"interview_date\"] = timestamp.values[0]\n",
    "for val in T2_missing_subs:\n",
    "    timestamp = df_visit.loc[pd.IndexSlice[(val, \"T1\")], \"interview_date\"]\n",
    "    df_visit.loc[pd.IndexSlice[(val, \"T2\")], \"interview_date\"] = timestamp + relativedelta(months=6)\n",
    "for val in T3_missing_subs:\n",
    "    timestamp = df_visit.loc[pd.IndexSlice[(val, \"T1\")], \"interview_date\"]\n",
    "    df_visit.loc[pd.IndexSlice[(val, \"T3\")], \"interview_date\"] = timestamp + relativedelta(months=12)    \n",
    "df_visit[df_visit.interview_date.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7f76a-aeaf-4848-92f8-efffb14bd9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_month(d1, d2):\n",
    "    return int((d1.year - d2.year) * 12 + d1.month - d2.month)\n",
    "\n",
    "def get_age(x):\n",
    "    dob = pd.to_datetime(df_ids[df_ids.banda_id == x.name[0]].dob.values[0])\n",
    "    return diff_month(x.interview_date, dob)\n",
    "\n",
    "def get_gender(x):\n",
    "    val = banda_df_ids[banda_df_ids.banda_id == x.name[0]].sex.values\n",
    "    # demo = dfs_assess_par[\"demo\"] \n",
    "    # demo = demo[demo.visit == \"T1\"]\n",
    "    # val = demo[demo.src_subject_id == x.name[0]].demo_child_gender_i.values\n",
    "    if len(val):\n",
    "        val = val[0]\n",
    "        return \"F\" if val < 0.5 else \"M\"\n",
    "    return np.nan\n",
    "    \n",
    "df_visit[\"interview_age\"] = df_visit.dropna(subset=[\"interview_date\"]).apply(get_age, axis=1)\n",
    "df_visit[\"gender\"] = df_visit.apply(get_gender, axis=1)\n",
    "df_visit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ca998b-a687-45c6-afbe-7dc531a88518",
   "metadata": {},
   "source": [
    "#### Create a flat visit mapping table that includes GUIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ae5b4-e753-455e-8808-f7003bf014a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_flatten = pd.concat((df_visit.index.to_frame(), df_visit), axis=1)\n",
    "subj_flatten[\"subjectkey\"] = subj_flatten.src_subject_id.map(subjmap)\n",
    "subj_flatten = subj_flatten[[\"subjectkey\", \"src_subject_id\", \"interview_date\", \"interview_age\", \"gender\", \"visit\"]]\n",
    "\n",
    "def get_numeric_id(x):\n",
    "    subjects = df_banda[df_banda.src_subject_id==x][[\"src_subject_id\", \"subject_id\"]].drop_duplicates().subject_id.unique().tolist()\n",
    "    if len(subjects) > 1:\n",
    "        return np.nan\n",
    "    return subjects.pop()\n",
    "subj_flatten[\"numeric_id\"] = subj_flatten.src_subject_id.apply(get_numeric_id)\n",
    "subj_flatten.interview_date = subj_flatten.interview_date.apply(lambda x: datetime.strftime(x, \"%m/%d/%Y\"))\n",
    "                                  \n",
    "subj_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c409ab-45d0-4f55-98a9-9ac2c97f438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_subj_info(df):\n",
    "    \"\"\"Add NDAR variables to response tables\"\"\"\n",
    "    foo = df.set_index(keys=[\"src_subject_id\", \"visit\"])\n",
    "    for key in [\"subject_id\", \"interview_date\"]:\n",
    "        if key in foo.columns:\n",
    "            foo = foo.drop(columns=[key])\n",
    "    if \"respondent\" in foo.columns:\n",
    "        foo = foo[[\"respondent\"] + [var for var in foo.columns.tolist() if var != \"respondent\"]]\n",
    "    return pd.concat((subj_flatten.drop(columns=[\"numeric_id\"]), foo), axis=1, join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80858ed-bd0f-4f51-bfd6-8d5adb50c998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a unique output dir for each run of this script\n",
    "outdir = (Path() / f\"output-{datetime.now()}\")\n",
    "outdir.mkdir()\n",
    "outdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee9d96c-61da-45bf-9905-89bcd0668b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remap Redcap names to NDA structure names\n",
    "remapkey = {\"handed\": \"chaphand\", \n",
    "            \"neo\": \"nffi\",\n",
    "            \"ksads_diagnoses01\": \"diagnoses01\",\n",
    "            \"ksads_diagnosesp201\": \"diagnosesp201\"}\n",
    "\n",
    "def insert_dictname(outfile, dictname, version):\n",
    "    \"\"\"Insert the structure name into the CSV to comply with NDA structures\"\"\"\n",
    "    with open(outfile) as fp:\n",
    "        table = fp.readlines()\n",
    "    extra_commas = len([1 for v in table[0] if v == \",\"]) - 1\n",
    "    table.insert(0, f\"{dictname},{version}{',' * extra_commas}\\n\")\n",
    "    with open(outfile, \"w\") as fp:\n",
    "        fp.writelines(table)\n",
    "\n",
    "\n",
    "def write_csv(dfs, outdir, structnamemap={}):\n",
    "    \"\"\"Write a dataframe into the corresponding NDA structure csv\"\"\"\n",
    "    for key, df in dfs.items():\n",
    "        dfkey = combine_subj_info(df)\n",
    "        structname = structnamemap.get(key)\n",
    "        if structname is None:\n",
    "            if key in remapkey:\n",
    "                key = remapkey[key]\n",
    "            for val in assess_map:\n",
    "                if key.split(\"_\")[0] in val:\n",
    "                    structname = val\n",
    "                    continue\n",
    "            if structname is None:\n",
    "                raise ValueError(f\"Could not find {key} in assess_map\")\n",
    "        print(key, key.split(\"_\")[0], structname)\n",
    "        outfile = outdir / f\"{structname}.csv\"\n",
    "        if outfile.exists():\n",
    "            df = pd.read_csv(outfile, skiprows=[0])\n",
    "            dfkey = pd.concat((df, dfkey), axis=0, ignore_index=True)\n",
    "        dfkey.reset_index(drop=True, inplace=True)\n",
    "        if \"respondent\" in dfkey:\n",
    "            dfkey = dfkey.sort_values([\"respondent\", \"visit\", \"src_subject_id\"],\n",
    "                                      ignore_index=True)\n",
    "        else:\n",
    "            dfkey = dfkey.sort_values([\"visit\", \"src_subject_id\"],\n",
    "                                      ignore_index=True)\n",
    "        dfkey = dfkey.convert_dtypes()\n",
    "        if \"ksads\" in str(outfile):\n",
    "            dfkey.to_csv(outfile, index=False, na_rep=0)\n",
    "        else:\n",
    "            dfkey.to_csv(outfile, index=False, na_rep=999)\n",
    "        dictname, version = structname[:-2],structname[-2:]\n",
    "        print(dictname, version)\n",
    "        insert_dictname(outfile, dictname, int(version))\n",
    "        structname = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f441421d-0bdb-45b2-b6cd-6a5f3544cd70",
   "metadata": {},
   "source": [
    "#### Write data into existing or new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b83f31-652a-4e28-a269-5e249591c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv(dfs_assess, outdir)\n",
    "write_csv(dfs_assess_par, outdir)\n",
    "write_csv(df_other, outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd60b28-73d7-4738-b962-044f4f1a710c",
   "metadata": {},
   "source": [
    "### Process other data sources\n",
    "\n",
    "1. NIH toolbox\n",
    "1. STRAIN \n",
    "1. Penn cognitive battery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10483edc-4d00-4f2c-b238-fd1e6b80865c",
   "metadata": {},
   "source": [
    "#### NIH toolbox\n",
    "\n",
    "Multiple CSVs refer to the different sources that were accumulated between\n",
    "the receiving server and exported directly from the IPads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b9a10d-8b1e-4f62-8792-c996851b5ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nihcsvs = glob(\"../../../../../NIHtoolbox/*Scores*csv_*\") + \\\n",
    "        glob(\"../../../../../remaining_nihtoolbox_data/*/*Scores.csv\")\n",
    "len(nihcsvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a43736-ced1-4aa7-86ed-49d9e439c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "nih_dfs = []\n",
    "for filename in nihcsvs:\n",
    "    nih_dfs.append(pd.read_csv(filename))\n",
    "df_nih = pd.concat(nih_dfs, axis=0, ignore_index=True).drop_duplicates()\n",
    "df_nih.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2880118e-8dfe-460a-856b-73a3134ad14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_subject_id(x):\n",
    "    if x in [\"Gyz\", \"Train123\", \"Test\", \"Testt\", \"Viv\", \"Test6\", \"Test 4\", \"Test3\"]:\n",
    "        return np.nan\n",
    "    x = int(x)\n",
    "    if x < 100 or x > 900 or not x in banda_df_ids.subject_id.dropna().values.astype(int):\n",
    "        return np.nan\n",
    "    return x\n",
    "df_nih['subject_id'] = df_nih.PIN.dropna().apply(to_subject_id)\n",
    "df_nih[\"src_subject_id\"] = df_nih.subject_id.dropna().apply(lambda x: banda_df_ids[banda_df_ids.subject_id == int(x)].banda_id.values[0])\n",
    "df_nih = df_nih.dropna(subset=[\"src_subject_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f674908-827d-4bbf-9249-f40e552d63ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nih[['PIN', \"src_subject_id\", 'Inst', 'RawScore', 'ItmCnt', 'DateFinished', \n",
    "        'Uncorrected Standard Score', 'Age-Corrected Standard Score',\n",
    "        'Fully-Corrected T-score', 'National Percentile (age adjusted)']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d6ea5a-50a5-4e7a-b08b-1719ac0169cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nih.Inst.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6c0266-e2c2-46f4-bf8c-9ac0cdec52fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nihtasks = [\"List\", \"Flanker\", \"Dimensional\", \"Oral\", \"Pattern\"]\n",
    "columns = ['RawScore', 'ItmCnt', 'Uncorrected Standard Score', 'Age-Corrected Standard Score',\n",
    "        'Fully-Corrected T-score']\n",
    "\n",
    "taskmap = {}\n",
    "for val in df_nih.Inst.unique():\n",
    "    if \"Flanker\" in val:\n",
    "        task = splitname = \"Flanker\"\n",
    "    else:\n",
    "        splitname = \".\".join(val.split(\"Toolbox \")[1].split(\"Test\")[0].split() + [\"Test\"])\n",
    "        task = splitname.split(\".\")[0]\n",
    "    taskmap[task] = splitname\n",
    "\n",
    "def name2var(task, col):\n",
    "    if col == \"RawScore\":\n",
    "        col = \"Raw Score\"\n",
    "    cols = col.replace(\"T-s\", \"T S\").replace(\"-\", \" \").split()\n",
    "    return \".\".join([taskmap[task]] + cols)\n",
    "\n",
    "def get_tbx_df(df_nih, task, colmap):\n",
    "    df_nih_task = df_nih[df_nih.Inst.dropna().apply(lambda x: task in x)]\n",
    "    df_nih_task = df_nih_task[colmap.keys()]\n",
    "    df_nih_task.columns = colmap.values()\n",
    "    df_nih_task[\"visit\"] = \"T1\"\n",
    "    df_nih_task = df_nih_task.drop_duplicates()\n",
    "    return df_nih_task.drop_duplicates(subset=[\"src_subject_id\", \"visit\"])\n",
    "\n",
    "df_nihtbx = {}\n",
    "for task in nihtasks:\n",
    "    colmap = {\"Inst\": \"version_form\", \"src_subject_id\": \"src_subject_id\"}\n",
    "    for col in columns:\n",
    "        banda_var = name2var(task, col)\n",
    "        info_idx = var_map[banda_var]\n",
    "        df2use = df_maps[info_idx]\n",
    "        row = df2use[df2use[\"banda_var\"] == banda_var]\n",
    "        ndar_var = row.ndar_var\n",
    "        colmap[col] = ndar_var.values[0]\n",
    "    key = row.ndar_structure.values[0]\n",
    "    df_nihtbx[key] = get_tbx_df(df_nih, task, colmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f84d7c-8ed7-47ab-adda-f483104adcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for grp, subdf in df_nihtbx['lswmt01'].groupby(\"src_subject_id\"):\n",
    "    if subdf.shape[0] > 1:\n",
    "        print(grp, subdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf724d0-f3a8-41e2-bf29-d840fcdb8cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv(df_nihtbx, outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46563ba4-ab3f-4d4c-ac2e-36bd40a4fdd8",
   "metadata": {},
   "source": [
    "#### Convert STRAIN data into NDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952202d1-0172-4efd-b78f-f7776dd16c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strain = pd.read_csv(\"STRAIN-BANDA_09-06-22.csv\")\n",
    "df_strain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad87b92-8b55-44df-8dbd-cfc928424c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_subject_id(x):\n",
    "    if \"test\" in x:\n",
    "        return np.nan\n",
    "    x = int(x)\n",
    "    if x < 100 or x > 900 or not x in banda_df_ids.subject_id.dropna().values.astype(int):\n",
    "        return np.nan\n",
    "    return x\n",
    "columns = df_strain.columns[5:]\n",
    "df_strain['subject_id'] = df_strain.ID.dropna().apply(to_subject_id)\n",
    "df_strain[\"src_subject_id\"] = df_strain.subject_id.dropna().apply(lambda x: banda_df_ids[banda_df_ids.subject_id == int(x)].banda_id.values[0])\n",
    "df_strain = df_strain.dropna(subset=[\"src_subject_id\"])\n",
    "colmap = {\"src_subject_id\": \"src_subject_id\"}\n",
    "varnomap = []\n",
    "for banda_var in columns:\n",
    "    if banda_var not in var_map:\n",
    "        varnomap.append(banda_var)\n",
    "        continue\n",
    "    info_idx = var_map[banda_var]\n",
    "    df2use = df_maps[info_idx]\n",
    "    row = df2use[df2use[\"banda_var\"] == banda_var]\n",
    "    ndar_var = row.ndar_var\n",
    "    colmap[banda_var] = ndar_var.values[0]\n",
    "print(\"Missing in NDAR: \", varnomap)\n",
    "df_strain = df_strain[colmap.keys()]\n",
    "df_strain.columns = colmap.values()\n",
    "df_strain[\"respondent\"] = \"Child\"\n",
    "df_strain[\"visit\"] = \"T1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a649135d-92d7-4790-8b6b-0246710c7aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515bf8ce-3c7f-4032-a936-9ae77704c504",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab7990d-ff71-433a-8f17-24ebd7f469ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv({\"strain01\": df_strain}, outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2496522d-c476-451c-9704-612c501c3443",
   "metadata": {},
   "source": [
    "#### Convert Penn Toolbox data into NDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa5056-c878-4579-a5c7-81367a2c322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penn = pd.read_csv(\"penn_all_banda214.csv\")\n",
    "for key in [\"PMAT24_A.PMAT24_A_RTCR\", \"PMAT24_A.PMAT24_A_RTER\", \"PMAT24_A.PMAT24_A_RTTO\"]:\n",
    "        df_penn[key] = df_penn[key].dropna().apply(lambda x: np.round(x))\n",
    "df_penn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df1f8d9-02b5-4e11-91c7-204988aaa841",
   "metadata": {},
   "outputs": [],
   "source": [
    "penntasks = [\"Penn Word Memory Test\", \"Penn Matrix Reasoning Test\", \"Penn Emotion Regonition Test\",\n",
    "            \"Delay Discounting Task\"]\n",
    "pennvars = [\"KCPW_A.\", \"PMAT24_A.\", \"ER40_D.\", \"DDISC.\"]\n",
    "intvars = [\"er40_c_cr\", \"er40_d_fc\", \"er40_d_mc\", \"er40_c_ang\",\n",
    "           \"er40_c_fear\", \"er40_c_hap\", \"er40_c_noe\", \"er40_c_sad\"]\n",
    "\n",
    "def get_task_df(df_p, colmap):\n",
    "    df_penn_task = df_p[colmap.keys()].copy()\n",
    "    df_penn_task.columns = colmap.values()\n",
    "    df_penn_task[\"respondent\"] = \"Child\"\n",
    "    df_penn_task[\"visit\"] = \"T1\"\n",
    "    df_penn_task = df_penn_task.drop_duplicates()\n",
    "    return df_penn_task.drop_duplicates(subset=[\"src_subject_id\", \"visit\"])\n",
    "\n",
    "df_penntbx = {}\n",
    "for task in pennvars:\n",
    "    colmap = {\"banda_id\": \"src_subject_id\"}\n",
    "    taskcols = [var for var in df_penn.columns if var.startswith(task)]\n",
    "    for banda_var in taskcols:\n",
    "        if banda_var not in var_map:\n",
    "            continue\n",
    "        info_idx = var_map[banda_var]\n",
    "        df2use = df_maps[info_idx]\n",
    "        row = df2use[df2use[\"banda_var\"] == banda_var]\n",
    "        ndar_var = row.ndar_var\n",
    "        colmap[banda_var] = ndar_var.values[0]\n",
    "    key = row.ndar_structure.values[0]\n",
    "    df_penntbx[key] = get_task_df(df_penn, colmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79928849-78ce-4f16-b89d-b2c0421e558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penntbx[key].head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7424ab30-1270-44f5-9f9d-557be55c742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv(df_penntbx, outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c807774-3793-4af6-816a-c43ecd9423e7",
   "metadata": {},
   "source": [
    "### Write subject structure (ndar_subject01) with diagnostic groups\n",
    "\n",
    "**Anxious:** current diagnosis of at least one of {anx} disorders and no current dx of any {dep} disorders\n",
    "\n",
    "- IF diagnoses_current_1=1; AND dx_current_2__{anx}=1; AND NOT dx_current_2__{dep}=1\n",
    "- Where {anx} = agorophobia; gen_anxiety; panic; sep_anxiety; social_phobia; specific_phobia\n",
    "- No lifetime consideration\n",
    "\n",
    "**Depressed:** current dx of at least one {dep} disorders\n",
    "\n",
    "- IF diagnoses_current_1=1; AND dx_current_2__{dep}=1\n",
    "- Where {dep} = adj_depressed; depressive_nos; dysthymia; mdd\n",
    "- No {anx} current nor any lifetime consideration\n",
    "\n",
    "**Control:** No current disorder, no lifetime {anx} nor {dep} disorder\n",
    "\n",
    "- IF diagnoses_current_1=0; AND diagnoses_lifetime_1=0; OR \n",
    "- IF diagnoses_current_1=0; AND diagnoses_lifetime_1=1 NOT {anx} nor {dep} lifetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4959a6d5-5bd7-41d0-9ba5-4a7a77a34c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_map = {1: \"American Indian/Alaskan Native\",\n",
    "            2: \"Asian\",\n",
    "            3: \"Black or African American\", \n",
    "            4: \"Hawaiian or Pacific Islander\",\n",
    "            5: \"White\",\n",
    "            6: \"More than one race\",\n",
    "            7: \"Unknown or not reported\"}\n",
    "ethnic_map = {1: \"Hispanic or Latino\",\n",
    "              2: \"Not Hispanic or Latino\",\n",
    "              3: \"Unknown or not reported\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc677b-d1d1-4cc6-b36d-9a87ce16b640",
   "metadata": {},
   "outputs": [],
   "source": [
    "anx = [f\"dx_current_2___{anx}\" for anx in [\"agoraphobia\", \"gen_anxiety\", \"panic\", \"sep_anxiety\", \"social_phobia\", \"specific_phobia\", \"other_spec_anxiety\"]]\n",
    "dep = [f\"dx_current_2___{dep}\" for dep in [\"adj_depressed\", \"depressive_nos\", \"dysthymia\", \"mdd\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a17fdb-c6fa-4f8b-b688-2cfc5fcb9ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shortname = \"ndar_subject01\"\n",
    "\n",
    "fields = [\"phenotype\", \"phenotype_description\"]\n",
    "\n",
    "df_subject = banda_df_ids[[\"banda_id\", \"race\", \"ethnicity\"]].copy()\n",
    "df_subject.race = df_subject.race.apply(lambda x: race_map[x])\n",
    "df_subject.ethnicity = df_subject.ethnicity.apply(lambda x: ethnic_map[x])\n",
    "df_subject.columns = [\"src_subject_id\", \"race\", \"ethnic_group\"]\n",
    "df_subject[\"phenotype\"] = \"Unknown\"\n",
    "df_subject[\"phenotype_description\"] = \"\"\"dx= partial remission (3) or definite (4)\n",
    "{anx}= agorophobia; gen_anxiety; panic; sep_anxiety; social_phobia; specific_phobia;\n",
    "other_spec_anxiety\n",
    "{dep}= adj_depressed; depressive_nos; dysthymia; mdd\n",
    "**Anxiety:** at least one current {anx} dx and NO current {dep} dx\n",
    "- IF diagnoses_current_1=1; AND dx_current_2__{anx}=1; AND NOT dx_current_2__{dep}=1\n",
    "- No lifetime dx consideration for label\n",
    "**Depression:** at least one current {dep} dx, MUST be present at time of interview to meet\n",
    "label criteria (eg, met dx threshold for major depressive episode at intake)\n",
    "- IF diagnoses_current_1=1; AND dx_current_2__{dep}=1\n",
    "- {anx} dx is NOT exclusionary for label; No lifetime dx consideration for label\n",
    "**Control:** No current dx, no current or lifetime {anx} nor {dep} dx\n",
    "- IF diagnoses_current_1=0; AND diagnoses_lifetime_1=0; OR\n",
    "- IF diagnoses_current_1=0; AND diagnoses_lifetime_1=1 NOT {anx} nor {dep} dx lifetime\"\"\"\n",
    "anx_subs = ((df_ids[\"diagnoses_current_1\"] == 1) & (df_ids[anx].sum(axis=1) > 0) & (df_ids[dep].sum(axis=1) == 0))\n",
    "anx_idx = df_ids[anx_subs].banda_id\n",
    "dep_subs = ((df_ids[\"diagnoses_current_1\"] == 1) & (df_ids[dep].sum(axis=1) > 0))\n",
    "dep_idx = df_ids[dep_subs].banda_id\n",
    "ctrl_subs = ((df_ids[\"diagnoses_current_1\"] == 0) & (df_ids[\"dx_lifetime_1\"] == 0)) | \\\n",
    "            ((df_ids[\"diagnoses_current_1\"] == 0) & (df_ids[\"dx_lifetime_1\"] == 1) \\\n",
    "             & (~ ((df_ids[dep].sum(axis=1) > 0) | (df_ids[anx].sum(axis=1) > 0))))\n",
    "ctrl_idx = df_ids[ctrl_subs].banda_id\n",
    "df_subject.loc[df_subject.src_subject_id.isin(anx_idx), \"phenotype\"] = \"Anxiety\"\n",
    "df_subject.loc[df_subject.src_subject_id.isin(dep_idx), \"phenotype\"] = \"Depression\"\n",
    "df_subject.loc[df_subject.src_subject_id.isin(ctrl_idx), \"phenotype\"] = \"Control\"\n",
    "df_subject[\"visit\"] = \"T1\"\n",
    "df_subject[\"twins_study\"] = \"No\"\n",
    "df_subject[\"sibling_study\"] = \"No\"\n",
    "df_subject[\"family_study\"] = \"No\"\n",
    "df_subject[\"sample_taken\"] = \"No\"\n",
    "df_subject.sort_values(\"src_subject_id\", inplace=True)\n",
    "df_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97809a2d-0cbc-42bf-85eb-a4052325abf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subject.phenotype.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10acf335-3da0-47a6-a0d5-25239ecb66c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unk = df_ids.loc[df_ids.banda_id.isin(df_subject[df_subject.phenotype == \"Unknown\"].src_subject_id), [\"banda_id\", \"diagnoses_current_1\"] + anx + dep]\n",
    "df_unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d7110-a26c-487d-a152-b6bd109d6eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv({shortname: df_subject}, outdir, {shortname: shortname})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e082955-6eb5-4750-acbc-2245c8689d26",
   "metadata": {},
   "source": [
    "#### Count all the outputs produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622b5a2b-6421-437c-8bf8-7b7a2792aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcsvs = sorted(glob(str(outdir / \"*.csv\")))\n",
    "len(outcsvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4dc6fc-94a7-4c72-88c0-bfe3e3c17172",
   "metadata": {},
   "source": [
    "Download the required data elements for each NDA structure\n",
    "and rewrite the CSVs with missing data filled in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3782dce5-0b15-4ca7-b4db-7f1fffa984a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(\"required_de.json\").exists():    \n",
    "    with open(\"required_de.json\") as fp:\n",
    "        required_des = json.load(fp)\n",
    "else:\n",
    "    ndar_dicts = [Path(val).name.split(\".csv\")[0] for val in outcsvs]\n",
    "    required_des = {}\n",
    "    for shortname in ndar_dicts:\n",
    "        info = requests.get(f\"https://nda.nih.gov/api/datadictionary/datastructure/{shortname}\").json()\n",
    "        required_des[shortname] = {}\n",
    "        for de in info[\"dataElements\"]:\n",
    "            if de[\"required\"] == \"Required\":\n",
    "                required_des[shortname][de[\"name\"]] = de[\"aliases\"]\n",
    "    with open(\"required_de.json\", \"wt\") as fp:\n",
    "        json.dump(required_des, fp, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206e2798-afcb-4fac-be15-32fad073671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing(filename, shortname, elements):\n",
    "    df_name = pd.read_csv(filename, na_values=[999], skiprows=[0])\n",
    "    for el in elements:\n",
    "        df_name[el] = np.nan\n",
    "    df_name = df_name.convert_dtypes()\n",
    "    df_name.to_csv(filename, index=False, na_rep=999)\n",
    "    insert_dictname(filename, shortname[:-2], int(shortname[-2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c602e-7878-4cb1-9b22-cc6d0b5bc2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, req_els in required_des.items():\n",
    "    csvname = outdir / f\"{name}.csv\"\n",
    "    df_name = pd.read_csv(csvname, na_values=[999], skiprows=[0])\n",
    "    missing_de = []\n",
    "    for val, aliases in req_els.items():\n",
    "        if val in df_name.columns:\n",
    "            continue\n",
    "        if any([alias in df_name.columns for alias in aliases]):\n",
    "            continue\n",
    "        missing_de.append(val)\n",
    "    print(name, missing_de)\n",
    "    if missing_de:\n",
    "        add_missing(csvname, name, missing_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29ca55c-9978-4927-8086-3ba617c1c301",
   "metadata": {},
   "source": [
    "### Process composite scores\n",
    "\n",
    "1. BISBAS\n",
    "1. MFQ\n",
    "1. NEO FFI\n",
    "1. RBQA\n",
    "1. RCADS\n",
    "1. SHAPS\n",
    "1. STAI\n",
    "1. CBCL\n",
    "1. MASQ\n",
    "1. RMBI\n",
    "\n",
    "Each section contains code to remap NDA data elements to the scoring needed\n",
    "to generate the composite scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cc8b1f-1508-4310-991d-09242f627224",
   "metadata": {},
   "source": [
    "#### Bisbas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10474a9-9bfe-4e6c-8b99-fa8cb1602ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvname = outdir / \"bisbas01.csv\"\n",
    "bisbas_dict = dict(bissc_total=[2,8,13,16,19,22,24],\n",
    "                   bas_drive=[3, 9, 12, 21],\n",
    "                   bas_fs=[5, 10, 15, 20],\n",
    "                   bas_rr=[4, 7, 14, 18, 23])\n",
    "bisbas_df = pd.read_csv(csvname, na_values=[999], skiprows=[0])\n",
    "for key, val in bisbas_dict.items():\n",
    "    columns = [f\"bisbas{v}\" for v in val]\n",
    "    bisbas_df[key] = bisbas_df[columns].sum(axis=1)\n",
    "bisbas_df[\"version_form\"] = \"2013\"\n",
    "bisbas_df = bisbas_df.convert_dtypes()\n",
    "bisbas_df.to_csv(csvname, index=False)\n",
    "display(bisbas_df.head())\n",
    "insert_dictname(csvname, \"bisbas\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c737cf52-e895-4c4d-83bf-742a8e0e40ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### MFQ (LONG VERSION)\n",
    "\n",
    "For adolescent self-report:\n",
    "\n",
    "`mfqtot = sum(items[1-33])`\n",
    "\n",
    "For parent report on adolescent:\n",
    "\n",
    "`mfqtot = sum(items[1-34])`\n",
    "\n",
    "NDA codes this questionnaire differently from the standard questionnaire. \n",
    "\n",
    "- MFQ score scale: Not True = 0, Sometimes = 1, True =2\n",
    "- NDA score scale: Not True = 1, Sometimes = 2, True =3\n",
    "\n",
    "Hence for NDA total score a 1 has to be subtracted from each entry before computing total score. See Daviss et al. for details. https://acamh.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-7610.2006.01646.x?casa_token=dMiq1Fx3CqAAAAAA:haUEBr0WtiMdBzrboWFZrEEAO6cJzojckjIW8qLD0_nB94ckAXiuLvtfdZ2nwj60hLu3PhxhVEPLGg  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68876c8c-df21-49a2-91e5-3f429fa49349",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvname = outdir / \"mfq01.csv\"\n",
    "mfq_df = pd.read_csv(csvname, na_values=[999], skiprows=[0])\n",
    "columns = [f\"mfq{v}\" for v in range(1, 35)]\n",
    "mfq_df[\"mfqtot\"] = (mfq_df[columns] - 1).sum(axis=1)\n",
    "mfq_df = mfq_df.convert_dtypes()\n",
    "mfq_df.to_csv(csvname, index=False, na_rep=999)\n",
    "display(mfq_df.head())\n",
    "insert_dictname(csvname, \"mfq\", 1)\n",
    "mfq_df[[\"respondent\", \"mfqtot\"]].groupby(\"respondent\").hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf334845-0069-4e42-99b8-ea42f6e75d4d",
   "metadata": {},
   "source": [
    "#### NEO FFI\n",
    "\n",
    "reverse score: [1, 4, 7, 10] then sum remaining items of the 12 neo items.\n",
    "\n",
    "NDA Formula: \n",
    "\n",
    "```\n",
    "neo2_score_ne: reverse(nffi_1) + reverse(nffi_16) + reverse(nffi_31) + reverse(nffi_46) + nffi_6 + nffi_11 + \n",
    "nffi_21 + nffi_26 + nffi_36 + nffi_41 + nffi_51 + nffi_56\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f01f6-5790-4219-a965-c92841474cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvname = outdir / \"nffi01.csv\"\n",
    "nffi_df = pd.read_csv(csvname, na_values=[999], skiprows=[0])\n",
    "reverse_nffi = lambda x: 6 - x\n",
    "nffi_df[\"neo2_score_ne\"] = nffi_df[[\"neo_ne1\", \"neo_ne4\", \"neo_ne7\", \"neo_ne10\"]].apply(reverse_nffi).sum(axis=1) + \\\n",
    "     nffi_df[[\"neo_ne2\", \"neo_ne3\", \"neo_ne5\", \"neo_ne6\", \"neo_ne8\", \"neo_ne9\", \"neo_ne11\", \"neo_ne12\"]].sum(axis=1)\n",
    "nffi_df = nffi_df.convert_dtypes()\n",
    "nffi_df.to_csv(csvname, index=False, na_rep=999)\n",
    "display(nffi_df.head())\n",
    "insert_dictname(csvname, \"nffi\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee396e6-5a37-48bc-8943-32c6ad22f9bc",
   "metadata": {},
   "source": [
    "#### RBQA\n",
    "\n",
    "```\n",
    "rbqa_total (total score): sum for rbqa<items> for total \n",
    "rbqa_scale1 (unsafe sexual practices): 2, 12 \n",
    "rbqa_scale2 (aggressive and/or violent behaviors): 3, 4, 13 \n",
    "rbqa_scale3 (rule breaking): 8, 9, 17 \n",
    "rbqa_scale4 (dangerous, destructive, and/or illegal): 1, 10, 11, 16, 18, 20 \n",
    "rbqa_scale5 (self-injurious behaviors): 14, 15 \n",
    "rbqa_scale6 (alcohol and/or drug use): 5, 6, 7, 19 \n",
    "```\n",
    "\n",
    "See Auerbach and Gardiner, 2012: https://cdasr.mclean.harvard.edu/wp-content/uploads/2017/09/Auerbach_2012_BRT.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13fea9f-43c8-4c4a-ab75-dfe3f6d9466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvname = outdir / \"rbqa01.csv\"\n",
    "rbqa_df = pd.read_csv(csvname, na_values=[999], skiprows=[0])\n",
    "rbqa_dict = dict(rbqa_total=list(range(1, 21)),\n",
    "                 rbqa_scale1=[2, 12],\n",
    "                 rbqa_scale2=[3, 4, 13],\n",
    "                 rbqa_scale3=[8, 9, 17],\n",
    "                 rbqa_scale4=[1, 10, 11, 16, 18, 20],\n",
    "                 rbqa_scale5=[14, 15],\n",
    "                 rbqa_scale6=[5, 6, 7, 19]\n",
    "                )\n",
    "for key, val in rbqa_dict.items():\n",
    "    columns = [f\"rbqa{v}\" for v in val]\n",
    "    rbqa_df[key] = rbqa_df[columns].sum(axis=1)\n",
    "rbqa_df = rbqa_df.convert_dtypes()\n",
    "rbqa_df.to_csv(csvname, index=False, na_rep=999)\n",
    "display(rbqa_df.head())\n",
    "insert_dictname(csvname, \"rbqa\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be7cc1d-3597-4912-922f-118f272e0223",
   "metadata": {},
   "source": [
    "#### RCADS\n",
    "\n",
    "from Chorpita et al., (2015):\n",
    "```\n",
    "rcads_social_phob: 4, 7, 8, 12, 20, 30, 32, 38, 43 \n",
    "rcads_panic: 3, 14, 24, 26, 28, 34, 36, 39, 41 \n",
    "rcads_drs (major depression): 2, 6, 11, 15, 19, 21, 25, 29, 40, 47 \n",
    "rcads_sep_anx: 5, 9, 17, 18, 33, 45, 46 \n",
    "rcads_gen_anx: 1, 13, 22, 27, 35, 37 \n",
    "rcads_ocd: 10, 16, 23, 31, 42, 44 \n",
    "```\n",
    "\n",
    "Even though RCADS uses strings in the data dictionary it accepts ordinal values associated with it. \n",
    "\n",
    "In NDA definitions, but not computed here: `rcads_ars, rcads_total_int`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf3acf5-bc50-4093-858c-f17794dcbd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvname = outdir / \"rcads01.csv\"\n",
    "rcads_df = pd.read_csv(csvname, na_values=[999], skiprows=[0])\n",
    "rcads_dict = dict(rcads_tot=list(range(1, 48)),\n",
    "                  rcads_social_phob=[4, 7, 8, 12, 20, 30, 32, 38, 43],\n",
    "                  rcads_panic=[3, 14, 24, 26, 28, 34, 36, 39, 41],\n",
    "                  rcads_drs=[2, 6, 11, 15, 19, 21, 25, 29, 40, 47],\n",
    "                  rcads_sep_anx=[5, 9, 17, 18, 33, 45, 46],\n",
    "                  rcads_gen_anx=[1, 13, 22, 27, 35, 37],\n",
    "                  rcads_ocd=[10, 16, 23, 31, 42, 44],\n",
    "                )\n",
    "for key, val in rcads_dict.items():\n",
    "    columns = [f\"rcads_{v}\" for v in val]\n",
    "    rcads_df[key] = rcads_df[columns].sum(axis=1)\n",
    "rcads_df = rcads_df.convert_dtypes()\n",
    "rcads_df.to_csv(csvname, index=False, na_rep=999)\n",
    "display(rcads_df.head())\n",
    "insert_dictname(csvname, \"rcads\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de1745c-5cc7-4488-9cff-c8bc7900e4c1",
   "metadata": {},
   "source": [
    "#### SHAPS\n",
    "\n",
    "See Snaith et al., 1995 for original \"dichotomous scoring\"; Franken et al., 2007 for \"continuous scoring\".\n",
    "\n",
    "- Orig scale: 0 Strongly Disagree – 3 Strongly Agree \n",
    "- Scale for items that needs to be reversed: 0 Definitely Agree – 3 Strongly Disagree \n",
    "- NDA has received reversed data\n",
    "\n",
    "NDA Formulae: \n",
    "\n",
    "```\n",
    "shaps_total: sum all 14 items \n",
    "shaps_total_continuous: 56 – Total_score + 14\n",
    "shaps_total_dichotomous: Count number of items < 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e75527c-a89d-407b-96dc-a20a13ed4edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvname = outdir / \"shaps01.csv\"\n",
    "shaps_df = pd.read_csv(csvname, na_values=[999], skiprows=[0])\n",
    "shaps_df[\"shaps_total\"] = shaps_df[[f\"shaps{v}\" for v in range(1, 15)]].sum(axis=1)\n",
    "shaps_df[\"shaps_total_continuous\"] = 56 - shaps_df[\"shaps_total\"] + 14\n",
    "shaps_df[\"shaps_total_dichotomous\"] = (shaps_df[[f\"shaps{v}\" for v in range(1,15)]] < 3).sum(axis=1)\n",
    "shaps_df = shaps_df.convert_dtypes()\n",
    "shaps_df.to_csv(csvname, index=False, na_rep=999)\n",
    "display(shaps_df.head())\n",
    "insert_dictname(csvname, \"shaps\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c45819d-e777-484e-8bcf-de38bd2a5a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shaps_df[[\"shaps_total\", \"shaps_total_continuous\", \"shaps_total_dichotomous\"]].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8fcee-6125-4130-bd4d-78a045212ef9",
   "metadata": {},
   "source": [
    "#### STAI\n",
    "\n",
    "Adolescent NDA element formula for state and trait composites:\n",
    "\n",
    "```\n",
    "staiy_state: reverse(stai1) + reverse(stai2) + reverse(stai5) + reverse(stai_state8_i) + reverse(stai10) + reverse(stai11) + reverse(stai15) + reverse(stai16) + reverse(stai_state19_i) + reverse(stai20) + stai_state4_i + stai_state9_i + stai_state14_i + stai_state18_i + stai3 + stai6 + stai7 + stai12 + stai13 + stai17\n",
    "\n",
    "staiy_trait: reverse(stai21) + reverse(stai26) + reverse(stai27) + reverse(stai30) + reverse(stai33) + reverse(stai36) + reverse(stai39) + stai_trait2_i + stai_trait3_i + stai_trait5_i + stai_trait11_i + stai_trait14_i + stai_trait15_i + stai24 + stai28 + stai29 + stai32 + stai37 + stai38 + stai40\n",
    "```\n",
    "\n",
    "Parent NDA element formula for state and trait composites: \n",
    "\n",
    "```\n",
    "staiy_state: reverse(stai1) + reverse(stai2) + reverse(stai5) + reverse(stai8) + reverse(stai10) + reverse(stai11) + reverse(stai15) + reverse(stai16) + reverse(stai19) + reverse(stai20) + stai3 + stai4 + stai6 + stai7 + stai9 + stai12 + stai13 + stai14 + stai17 + stai18\n",
    "\n",
    "staiy_trait: reverse(stai21) + reverse(stai23) + reverse(stai26) + reverse(stai27) + reverse(stai30) + reverse(stai33) + reverse(stai34) + reverse(stai36) + reverse(stai39) + stai22 + stai24 + stai25 + stai28 + stai29 + stai31 + stai32 + stai35 + stai37 + stai38 + stai40\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd8e372-65cf-4123-817c-a8dfa38ae135",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvname = outdir / \"stai01.csv\"\n",
    "stai_df = pd.read_csv(csvname, na_values=[999], skiprows=[0])\n",
    "reverse_stai = lambda x: 5 - x\n",
    "stai_df[\"staiy_state\"] = (stai_df[[\"stai1\", \"stai2\", \"stai5\", \"stai_state8_i\", \"stai10\", \"stai11\", \"stai15\", \n",
    "                                   \"stai16\", \"stai_state19_i\", \"stai20\"]].apply(reverse_stai).sum(axis=1) + \\\n",
    "                          stai_df[[\"stai_state4_i\", \"stai_state9_i\", \"stai_state14_i\", \"stai_state18_i\", \n",
    "                                  \"stai3\", \"stai6\", \"stai7\", \"stai12\", \"stai13\", \"stai17\"]].sum(axis=1)) * \\\n",
    "                          (stai_df.respondent == \"Child\") + \\\n",
    "                         (stai_df[[\"stai1\", \"stai2\", \"stai5\", \"stai8\", \"stai10\", \"stai11\", \"stai15\", \n",
    "                                   \"stai16\", \"stai19\", \"stai20\"]].apply(reverse_stai).sum(axis=1) + \\\n",
    "                          stai_df[[\"stai3\", \"stai4\", \"stai6\", \"stai7\", \"stai9\", \"stai12\", \"stai13\", \n",
    "                                   \"stai14\", \"stai17\", \"stai18\"]].sum(axis=1)) * \\\n",
    "                          (stai_df.respondent == \"Parent\")\n",
    "stai_df[\"staiy_trait\"] = (stai_df[[\"stai21\", \"stai26\", \"stai27\", \"stai30\", \"stai33\", \"stai36\", \"stai39\"]].apply(reverse_stai).sum(axis=1) + \\\n",
    "                          stai_df[[\"stai_trait2_i\", \"stai_trait3_i\", \"stai_trait5_i\", \"stai_trait11_i\", \"stai_trait14_i\", \"stai_trait15_i\", \n",
    "                                   \"stai24\", \"stai28\", \"stai29\", \"stai32\", \"stai37\", \"stai38\", \"stai40\"]].sum(axis=1)) * \\\n",
    "                          (stai_df.respondent == \"Child\") + \\\n",
    "                         (stai_df[[\"stai21\", \"stai23\", \"stai26\", \"stai27\", \"stai30\", \"stai33\", \"stai34\", \n",
    "                                   \"stai36\", \"stai39\"]].apply(reverse_stai).sum(axis=1) + \\\n",
    "                          stai_df[[\"stai22\", \"stai24\", \"stai25\", \"stai28\", \"stai29\", \"stai31\", \"stai32\",\n",
    "                                   \"stai35\", \"stai37\", \"stai38\", \"stai40\"]].sum(axis=1)) * \\\n",
    "                          (stai_df.respondent == \"Parent\")\n",
    "stai_df = stai_df.convert_dtypes()\n",
    "stai_df.to_csv(csvname, index=False, na_rep=999)\n",
    "display(stai_df.head())\n",
    "insert_dictname(csvname, \"stai\", 1)\n",
    "stai_df[[\"respondent\", \"staiy_state\"]].groupby(\"respondent\").hist();\n",
    "stai_df[[\"respondent\", \"staiy_trait\"]].groupby(\"respondent\").hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1625b-5184-407f-bbaf-b4c7165820e8",
   "metadata": {},
   "source": [
    "#### CBCL \n",
    "\n",
    "**There are several other raw scores that are required in the NDA that do not appear below**\n",
    "\n",
    "Ref: Achenbach, 2001© CBCL/6-18 Syndrome Scales  \n",
    "\n",
    "8 syndrome scales:\n",
    "\n",
    "```\n",
    "cbcl_anxious_raw: [14, 29, 30, 31, 32, 33, 35, 45, 50, 52, 71, 91, 112]\n",
    "cbcl_withdrawn_raw: [5, 42, 65, 69, 75, 102, 103, 111]\n",
    "cbcl_somatic_c_raw: [47, 49, 51, 54, 56a-g]\n",
    "cbcl_social_p_raw: [11, 12, 25, 27, 34, 36, 38, 48, 62, 64, 79]\n",
    "cbcl_thought_raw: [9, 18, 40, 46, 58, 59, 60, 66, 70, 76, 83, 84, 85, 92, 100]\n",
    "cbcl_attention_raw: [1, 4, 8, 10, 13, 17, 41, 61, 78, 80]\n",
    "cbcl_rulebreak_raw: [2, 26, 28, 39, 43, 63, 67, 72, 73, 81, 82, 90, 96, 99, 101, 105, 106]\n",
    "cbcl_aggresive_raw: [3, 16, 19, 20, 21, 22, 23, 37, 57, 68, 86, 87, 88, 89, 94, 95, 97, 104]\n",
    "\n",
    "(NOT IN NDA) \n",
    "cbcl_other_raw: [6, 7, 15, 24, 44, 53, 55, 56h, 74, 77, 93, 98, 107, 108, 109, 110, 113]\n",
    "```\n",
    "\n",
    "Broader problem scales: \n",
    "\n",
    "```\n",
    "cbcl_internal_raw: [anxious_depressed, withdrawn_depressed, somatic_complaints]\n",
    "cbcl_external_raw: [rule_breaking, aggressive_behavior]\n",
    "cbcl_total_raw: sum ALL 8 syndrome scale totals + cbcl_other_raw items detailed above \n",
    "```\n",
    "\n",
    "Cbcl/6-18 DSM-Oriented Scales (6 scales) \n",
    "\n",
    "```\n",
    "cbcl_deresspr_raw: [5, 14, 18, 24, 35, 52, 54, 76, 77, 91, 100, 102, 103]\n",
    "cbcl_anxiety_raw: [11, 29, 30, 31, 45, 47, 50, 71, 112]\n",
    "cbcl_somatic_p_raw: [56a-g]\n",
    "cbcl_adhd_raw: [4, 8, 10, 41, 78, 93, 104]\n",
    "cbcl_oppositional_raw: [3, 22, 23, 86, 95]\n",
    "cbcl_conduct_raw: [15, 16, 21, 26, 28, 37, 39, 43, 57, 67, 72, 81, 82, 90, 97, 101, 106]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3d142b-e5f5-4226-93b7-7d04ad2175fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvname = outdir / \"cbcl01.csv\"\n",
    "cbcl_df = pd.read_csv(csvname, na_values=[999], skiprows=[0])\n",
    "cbcl_dict = dict(cbcl_anxious_raw=[14, 29, 30, 31, 32, 33, 35, 45, 50, 52, 71, 91, 112],\n",
    "                 cbcl_withdrawn_raw=[5, 42, 65, 69, 75, 102, 103, 111],\n",
    "                 cbcl_somatic_c_raw=[47, 49, 51, 54, \"56a\", \"56b\", \"56c\", \"56d\", \"56e\", \"56f\", \"56g\"],\n",
    "                 cbcl_social_p_raw=[11, 12, 25, 27, 34, 36, 38, 48, 62, 64, 79],\n",
    "                 cbcl_thought_raw=[9, 18, 40, 46, 58, 59, 60, 66, 70, 76, 83, 84, 85, 92, 100],\n",
    "                 cbcl_attention_raw=[1, 4, 8, 10, 13, 17, 41, 61, 78, 80],\n",
    "                 cbcl_rulebreak_raw=[2, 26, 28, 39, 43, 63, 67, 72, 73, 81, 82, 90, 96, 99, 101, 105, 106],\n",
    "                 cbcl_aggressive_raw=[3, 16, 19, 20, 21, 22, 23, 37, 57, 68, 86, 87, 88, 89, 94, 95, 97, 104],\n",
    "                 cbcl_other_raw=[6, 7, 15, 24, 44, 53, 55, \"56h\", 74, 77, 93, 98, 107, 108, 109, 110, \"113a\"],\n",
    "                 cbcl_depresspr_raw=[5, 14, 18, 24, 35, 52, 54, 76, 77, 91, 100, 102, 103],\n",
    "                 cbcl_anxiety_raw=[11, 29, 30, 31, 45, 47, 50, 71, 112],\n",
    "                 cbcl_somatic_p_raw=[\"56a\", \"56b\", \"56c\", \"56d\", \"56e\", \"56f\", \"56g\"],\n",
    "                 cbcl_adhd_raw=[4, 8, 10, 41, 78, 93, 104],\n",
    "                 cbcl_oppositional_raw=[3, 22, 23, 86, 95],\n",
    "                 cbcl_conduct_raw=[15, 16, 21, 26, 28, 37, 39, 43, 57, 67, 72, 81, 82, 90, 97, 101, 106]\n",
    "                )\n",
    "for key, val in cbcl_dict.items():\n",
    "    columns = [f\"cbcl{v}\" for v in val]\n",
    "    cbcl_df[key] = cbcl_df[columns].sum(axis=1)\n",
    "cbcl_df[\"cbcl_internal_raw\"] = cbcl_df[[\"cbcl_anxious_raw\", \"cbcl_withdrawn_raw\", \"cbcl_somatic_c_raw\"]].sum(axis=1)\n",
    "cbcl_df[\"cbcl_external_raw\"] = cbcl_df[[\"cbcl_rulebreak_raw\", \"cbcl_aggressive_raw\"]].sum(axis=1)\n",
    "cbcl_df[\"cbcl_total_raw\"] = cbcl_df[[\"cbcl_internal_raw\", \n",
    "                                     \"cbcl_social_p_raw\", \"cbcl_thought_raw\", \"cbcl_attention_raw\",\n",
    "                                     \"cbcl_external_raw\", \n",
    "                                     \"cbcl_other_raw\"]].sum(axis=1)\n",
    "cbcl_df = cbcl_df.drop([\"cbcl_other_raw\"], axis=1)\n",
    "cbcl_df = cbcl_df.convert_dtypes()\n",
    "cbcl_df.to_csv(csvname, index=False, na_rep=999)\n",
    "display(cbcl_df.head())\n",
    "insert_dictname(csvname, \"cbcl\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ebd947-470b-409f-ae80-4778806f00df",
   "metadata": {},
   "source": [
    "#### MASQ\n",
    "\n",
    "From Watson and Clark, 1991 62-item Short-form Scoring Key. See also Watson et al., 1995 J Abnormal Psychology \n",
    "\n",
    "masq2_score_aa (Anxious arousal subtest) \n",
    "masq2_score_ad (anhedonic depression subtest) \n",
    "masq2_score_gd (general distress anxious symptoms subtest) \n",
    "masq2_score_gdd (general distress depressive symptoms subtest) \n",
    "\n",
    "```\n",
    "masq2_score_aa: [2, 6, 13, 17, 19, 24, 28, 30, 37, 40, 42, 44, 46, 48, 52, 54, 62]\n",
    "masq2_score_ad: \n",
    "\n",
    "- REVERSE: 3, 7, 10, 15, 22, 27, 39, 43, 47, 49, 53, 56, 58, 60  \n",
    "- Normal: 18, 25, 33, 41, 50, 51, 57, 61 \n",
    "\n",
    "masq2_score_gd: [4, 8, 11, 14, 16, 20, 26, 32, 35, 55, 59]\n",
    "masq2_score_gdd: [1, 5, 9, 12, 21, 23, 29, 31, 34, 36, 38, 45]\n",
    "```\n",
    "\n",
    "```\n",
    "ANHEDONIC DEPRESSION\n",
    "AD= SYMP18 + SYMP25 + SYMP33 + SYMP41 + SYMPSO + SYMP51 +\n",
    "SYMP57 + SYMP61 + 84- (SYMP3 + SYMP7 + SYMPlO + SYMP15 +\n",
    "SYMP22 + SYMP27 + SYMP39 + SYMP43 + SYMP47 + SYMP49 + SYMP53 +\n",
    "SYMP56 + SYMP58 + SYMP60);\n",
    "```\n",
    "\n",
    "Note: scl14 is a string in NDA. The MASQ structure does not directly conform to the 62 item scale. \n",
    "\n",
    "For the masq2_score_ad composite, it:\n",
    "\n",
    "1. Sums all non-reverse scored items (Sum A) and then adds 84 \n",
    "1. For reverse scored items it takes 6 - item score, then sums these (Sum B)\n",
    "1. Then it subtracts Sum B from Sum A ([Sum A + 84] - Sum B)\n",
    "\n",
    "MASQ required a second level of remapping since the names in NDA differ\n",
    "significantly from the names in the MASQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866f6746-d659-4257-9bd5-f223aa7d08c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "banda2nda_masq_map = pd.read_csv(\"masq_summary_key.csv\")\n",
    "banda2nda_masq_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0030b5cc-e827-4dc5-abb5-2d15f9e38701",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvname = outdir / \"masq01.csv\"\n",
    "masq_df = pd.read_csv(csvname, na_values=[999], skiprows=[0])\n",
    "masq_dict = dict(masq2_score_aa=[2, 6, 13, 17, 19, 24, 28, 30, 37, 40, 42, 44, 46, 48, 52, 54, 62],\n",
    "                 masq2_score_gd=[4, 8, 11, 14, 16, 20, 26, 32, 35, 55, 59],\n",
    "                 masq2_score_gdd=[1, 5, 9, 12, 21, 23, 29, 31, 34, 36, 38, 45]\n",
    "                )\n",
    "banda2nda_masq = dict(zip(banda2nda_masq_map.banda_var, banda2nda_masq_map.ndar_var))\n",
    "nda_masq_items = list(banda2nda_masq_map.ndar_var)\n",
    "recoded_df = masq_df[[\"subjectkey\", \"src_subject_id\",\"visit\"] + nda_masq_items].copy()\n",
    "masq_func_map = {'None': lambda x: x,\n",
    "                 'add 1': lambda x: x + 1,\n",
    "                 'add 1, reverse': lambda x: 6 - (x + 1), \n",
    "                 'reverse': lambda x: 6 - x,\n",
    "                 'convert from string': lambda x: {\"not at all\": 1, \"a little bit\": 2, \"moderately\": 3,\n",
    "                                                     \"quite a bit\": 4, \"extremely\": 5}[x]\n",
    "                }\n",
    "\n",
    "ndar_masq_xfm = dict(zip(banda2nda_masq_map.ndar_var, banda2nda_masq_map[\"Transform for subscale within NDA\"]))\n",
    "for key in nda_masq_items:\n",
    "    recoded_df[key] = recoded_df[key].apply(masq_func_map[ndar_masq_xfm[key]])\n",
    "for key, val in masq_dict.items():\n",
    "    columns = [banda2nda_masq[f\"masq{v}\"] for v in val]\n",
    "    masq_df[key] = recoded_df[columns].sum(axis=1)\n",
    "masq2_score_ad_reverse = [3, 7, 10, 15, 22, 27, 39, 43, 47, 49, 53, 56, 58, 60]\n",
    "masq2_score_ad_normal = [18, 25, 33, 41, 50, 51, 57, 61]\n",
    "masq_df[\"masq2_score_ad\"] = recoded_df[[banda2nda_masq[f\"masq{v}\"] for v in masq2_score_ad_normal]].sum(axis=1) + 84 \\\n",
    "                          - recoded_df[[banda2nda_masq[f\"masq{v}\"] for v in masq2_score_ad_reverse]].sum(axis=1)\n",
    "masq_df = masq_df.convert_dtypes()\n",
    "masq_df.to_csv(csvname, index=False, na_rep=999)\n",
    "display(masq_df.head())\n",
    "insert_dictname(csvname, \"masq\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17804a24-1d5f-4300-9814-2a4d849661b3",
   "metadata": {},
   "source": [
    "#### RMBI\n",
    "\n",
    "see Gladstone and Parker, 2005 for scores \n",
    "\n",
    "- Orig Scale: 0 No/Hardly Ever – 2 Yes/Most of Time \n",
    "- Reverse Scale: 2 No/Hardly Ever – 0 Yes/Most of Time \n",
    "\n",
    "reverse score items: 4, 5, 7, 11, 13, 15 \n",
    "\n",
    "For NDA: \n",
    "\n",
    "- For normal score items replace 3’s with 0 \n",
    "- For reverse score items use: max(0, item – 1) \n",
    "\n",
    "```\n",
    "rmbi_fi (fearful inhibition): 2, 4, 5, 9, 11, 15 \n",
    "rmbi_na (non-approach): 1, 6, 10, 16, 18 \n",
    "rmbi_ra (risk avoidance): 7, 8, 13 \n",
    "rmbi_ss (shyness and sensitivity): 3, 12, 14, 17 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44073f3-d25e-4078-9aae-a36098929eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvname = outdir / \"rmbi01.csv\"\n",
    "rmbi_df = pd.read_csv(csvname, na_values=[999], skiprows=[0])\n",
    "rmbi_dict = dict(rmbi_fi=[2, 4, 5, 9, 11, 15],\n",
    "                 rmbi_na=[1, 6, 10, 16, 18],\n",
    "                 rmbi_ra=[7, 8, 13],\n",
    "                 rmbi_ss=[3, 12, 14, 17],\n",
    "                 rmbi_total=list(range(1, 19))\n",
    "                )\n",
    "rmbi_items = [f\"rmbi{v}\" for v in range(1, 19)]\n",
    "recoded_df = rmbi_df[[\"subjectkey\", \"src_subject_id\",\"visit\"] + rmbi_items].copy()\n",
    "reverse_items = [f\"rmbi{v}\" for v in [4, 5, 7, 11, 13, 15]]\n",
    "recoded_df[reverse_items] = (recoded_df[reverse_items] - 1).copy().clip(lower=0)\n",
    "recoded_df[rmbi_items] = recoded_df[rmbi_items].applymap(lambda x: 0 if x == 3 else x)\n",
    "for key, val in rmbi_dict.items():\n",
    "    columns = [f\"rmbi{v}\" for v in val]\n",
    "    rmbi_df[key] = recoded_df[columns].sum(axis=1)\n",
    "rmbi_df = rmbi_df.convert_dtypes()\n",
    "rmbi_df.to_csv(csvname, index=False, na_rep=999)\n",
    "display(rmbi_df.head())\n",
    "insert_dictname(csvname, \"rmbi\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9976e98-a86a-47fa-89e4-d70dab85146e",
   "metadata": {},
   "source": [
    "### HACK/FIX NDA submission ISSUES\n",
    "\n",
    "In addition to the recoding needed to convert NDA data elements to relevant \n",
    "items for composite scoring described above, NDA often requires specific types\n",
    "of values to encode missing data. \n",
    "\n",
    "This section applies these NDA structure specific elements to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06e859-49aa-4b33-885c-2db36f819adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in [\"pwmt01\", \"pmat01\"]:\n",
    "    csvname = outdir / (fname + \".csv\")\n",
    "    df_name = pd.read_csv(csvname, skiprows=[0])\n",
    "    df_name.valid_code = df_name.valid_code.apply(lambda x: 'F' if x==\"999\" or x==\"0\" else x)\n",
    "    df_name = df_name.convert_dtypes()\n",
    "    df_name.to_csv(csvname, index=False, na_rep='')\n",
    "    insert_dictname(csvname, fname[:-2], int(fname[-2:]))\n",
    "\n",
    "fname = \"demographics02\"\n",
    "csvname = outdir / (fname + \".csv\")\n",
    "df_name = pd.read_csv(csvname, na_values=[999, \"999\", \"\"], skiprows=[0])\n",
    "df_name.demo_other_parent_educ = df_name.demo_other_parent_educ.apply(lambda x: np.nan if x==9 else x)\n",
    "df_name = df_name.convert_dtypes()\n",
    "df_name.to_csv(csvname, index=False, na_rep='')\n",
    "insert_dictname(csvname, fname[:-2], int(fname[-2:]))\n",
    "\n",
    "# 999 to empty\n",
    "affected = [\"er4001\", \"masq01\", \"mfq01\", \"nffi01\", \"pmat01\", \"rbqa01\", \"rcads01\", \"rmbi01\",\n",
    "            \"shaps01\", \"cssrs01\", \"stai01\", \"pwmt01\", \"fhs01\", \"chaphand01\"]\n",
    "\n",
    "for fname in affected:\n",
    "    csvname = outdir / (fname + \".csv\")\n",
    "    df_name = pd.read_csv(csvname, na_values=[999, \"999\", \"\"], skiprows=[0])\n",
    "    if fname == \"shaps01\":\n",
    "        df_name.shaps_total = df_name.shaps_total.apply(lambda x: 999 if np.isnan(x) else x)\n",
    "    if fname == \"masq01\":\n",
    "        for field in [\"masq2_score_aa\", \"masq2_score_ad\", \"masq2_score_gd\", \"masq2_score_gdd\"]:\n",
    "            df_name[field] = df_name[field].apply(lambda x: 999 if np.isnan(x) else x)\n",
    "    df_name = df_name.convert_dtypes()\n",
    "    df_name.to_csv(csvname, index=False, na_rep='')\n",
    "    insert_dictname(csvname, fname[:-2], int(fname[-2:]))\n",
    "\n",
    "\n",
    "\n",
    "for fname, field in [(\"lswmt01\", \"nihtbx_list_fullycorrected\"), \n",
    "                     (\"pcps01\", \"nihtbx_pattern_fullycorrected\"),\n",
    "                     (\"dccs01\", None)]:\n",
    "    csvname = outdir / (fname + \".csv\")\n",
    "    df_name = pd.read_csv(csvname, na_values=[999, \"999\", \"\"], skiprows=[0])\n",
    "    if field is not None:\n",
    "        df_name[field] = df_name[field].apply(lambda x: 0 if np.isnan(x) else x)\n",
    "        na_rep = ''\n",
    "    else:\n",
    "        na_rep = 0\n",
    "    df_name = df_name.convert_dtypes()\n",
    "    df_name.to_csv(csvname, index=False, na_rep=na_rep)\n",
    "    insert_dictname(csvname, fname[:-2], int(fname[-2:]))\n",
    "\n",
    "fname = \"tanner_sms01\"\n",
    "csvname = outdir / (fname + \".csv\")\n",
    "df_name = pd.read_csv(csvname, na_values=[999, \"999\", \"\"], skiprows=[0])\n",
    "df_name.tsmfemale = df_name.tsmfemale.apply(lambda x: 9999 if np.isnan(x) else x)\n",
    "df_name.tsmmale = df_name.tsmmale.apply(lambda x: 9999 if np.isnan(x) else x)\n",
    "df_name.tsmboth = df_name.tsmboth.apply(lambda x: 9998 if np.isnan(x) else x)\n",
    "df_name = df_name.convert_dtypes()\n",
    "df_name.to_csv(csvname, index=False, na_rep='')\n",
    "insert_dictname(csvname, fname[:-2], int(fname[-2:]))\n",
    "\n",
    "fname = \"cbcl01\"\n",
    "csvname = outdir / (fname + \".csv\")\n",
    "df_name = pd.read_csv(csvname, skiprows=[0])\n",
    "pr_cols = [var for var in df_name.columns if \"cbcl_pr\" in var]\n",
    "df_name = df_name.drop(pr_cols, axis=1)\n",
    "df_name = df_name.convert_dtypes()\n",
    "df_name.to_csv(csvname, index=False)\n",
    "insert_dictname(csvname, fname[:-2], int(fname[-2:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee1fc9-0621-4a4f-bd80-aefafedcd136",
   "metadata": {},
   "source": [
    "### Create missingness dataframe\n",
    "\n",
    "This indicates which assessments were missing for each subject at\n",
    "the different visits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e282e4-a042-4d60-8dc5-70b203649da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = Path(\"output-2022-10-27 01:11:31.202796\")\n",
    "outcsvs = sorted(glob(str(outdir / \"*.csv\")))\n",
    "len(outcsvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909b4f08-3381-423c-834e-fe6bb149d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing = pd.DataFrame()\n",
    "fields = [\"subjectkey\", \"src_subject_id\", \"visit\"]\n",
    "for csv in outcsvs:\n",
    "    df_new = pd.read_csv(csv, skiprows=[0])\n",
    "    structname = Path(csv).name.split(\".csv\")[0]\n",
    "    indices = [tuple(val) for val in df_new[fields].values.tolist()]\n",
    "    indices = pd.Index(indices).drop_duplicates()\n",
    "    indices.names = fields\n",
    "    df_missing = pd.concat((df_missing, \n",
    "                            pd.DataFrame(False, columns=[structname],\n",
    "                                         index=indices)),\n",
    "                            axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7ebd79-952c-4e1f-81f9-1f31309f7ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebf66da-e5fb-4b9f-be40-0829a765e79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.facecolor':'white'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e09080e-7e61-4542-a3d1-e9103ca38d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in df_missing.groupby(\"visit\"):\n",
    "    plt.figure(figsize=(10, 10*val[1].shape[0] / val[1].shape[1]))\n",
    "    hp = sns.heatmap(val[1].droplevel(\"visit\").fillna(True).sort_index(level=\"src_subject_id\"),\n",
    "                     cmap=\"binary\", linewidths=0.1,linecolor=\"gray\",\n",
    "                     cbar=None)\n",
    "    plt.title(f\"visit: {val[0]}\")\n",
    "    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "    plt.savefig(f\"{val[0]}.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d797eb-d5ee-48b2-9a34-aa5c74cd96d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_missing.fillna(True) == False).sort_index(level=[\"visit\", \"src_subject_id\"]).reset_index().to_csv(\"Completeness.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017b0e2c-8a8a-42ce-b483-a08f24aba55c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
